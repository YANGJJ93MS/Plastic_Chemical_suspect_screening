{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final update 2024-12-11\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import random as rd\n",
    "import matplotlib\n",
    "import warnings\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##detection frequency of thoes features with hits, include the biotransformation product and perform ms1 search again. in R\n",
    "##calculate standard deviations and covariance of those features among 200 serum samples\n",
    "##generate the final inclusion list for ms/ms acquisition\n",
    "##with the ms2, perform molecular networking to support the plastic and their biotrnasformation products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yangj\\AppData\\Local\\Temp\\ipykernel_25660\\1381856669.py:8: DtypeWarning: Columns (18,19,20,23,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  targetmzdat_pos = pd.read_csv('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Plastic_map_chemicallist_match/Plastic_Chemical_List_organic.csv')\n",
      "Matching compounds: 100%|██████████| 5144/5144 [00:36<00:00, 140.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3974, 410)\n",
      "(1121, 15)\n",
      "Number of columns retained after filtering: 104\n"
     ]
    }
   ],
   "source": [
    "# Load peak area data for positive ion mode after batch correction\n",
    "pos_peak_area_200 = pd.read_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/filtered_peak_area_for_bc_70.csv\")\n",
    "\n",
    "# Load target mz data\n",
    "targetmzdat_pos = pd.read_csv('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Plastic_map_chemicallist_match/Plastic_Chemical_List_organic.csv')\n",
    "#targetmzdat_pos = pd.read_csv('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Plastic_map_chemicallist_match/Plastic_Chemical_List_organic_withRTprediction.csv')\n",
    "targetmzdat_pos['mz'] = targetmzdat_pos['Monoisotopic_Mass_ready'] + 1.007825\n",
    "\n",
    "# Select relevant columns for inquiry\n",
    "peak_area_inquiry = pos_peak_area_200[['Average Rt(min)', 'Average Mz'] + [col for col in pos_peak_area_200.columns if col.startswith('BH')]]\n",
    "\n",
    "# Function to calculate ppm difference\n",
    "def ppm_difference(value1, value2):\n",
    "    return abs(value1 - value2) / value1 * 1e6\n",
    "\n",
    "ppm_threshold = 5\n",
    "matched_rows_pos = []\n",
    "matched_summary = []\n",
    "\n",
    "# MS1 search and matching\n",
    "for _, chem_row in tqdm(targetmzdat_pos.iterrows(), total=targetmzdat_pos.shape[0], desc=\"Matching compounds\"):\n",
    "    mw = chem_row['mz']\n",
    "    matches = peak_area_inquiry[peak_area_inquiry['Average Mz'].apply(lambda mz: ppm_difference(mz, mw) <= ppm_threshold)]\n",
    "    if not matches.empty:\n",
    "        # Append the matches along with the corresponding compound information\n",
    "        for _, match_row in matches.iterrows():\n",
    "            matched_rows_pos.append({\n",
    "                **match_row,\n",
    "                'Matched Compound': chem_row['SMILES'],\n",
    "                'PREFERRED NAME': chem_row.get('PREFERRED NAME', None),\n",
    "                'DTXSID_Hits': chem_row.get('DTXSID', None),\n",
    "                'SMILES_STD': chem_row.get('SMILES_ready', None),\n",
    "                'DTXSID': chem_row.get('DTXSID', None),\n",
    "                'Name': chem_row.get('NAME', None),\n",
    "                'Preferred Name': chem_row.get('PREFERRED NAME', None),\n",
    "                'InChiKey_origin': chem_row.get('INCHIKEY', None)\n",
    "            })\n",
    "        # Record summary statistics for each targeted mz, preserving multiple hits\n",
    "        matched_summary.append({\n",
    "            'Targeted MZ': mw,\n",
    "            'Matched Count': len(matches),\n",
    "            'Average Rt(min)': matches['Average Rt(min)'].tolist(),\n",
    "            'Average Mz': matches['Average Mz'].tolist(),\n",
    "            'Mean Value': matches.iloc[:, 3:].mean(axis=1).tolist(),\n",
    "            'Max Value': matches.iloc[:, 3:].max(axis=1).tolist(),\n",
    "            'Min Value': matches.iloc[:, 3:].min(axis=1).tolist(),\n",
    "            'Std Dev': matches.iloc[:, 3:].std(axis=1).tolist(),\n",
    "            'RSD (%)': ((matches.iloc[:, 3:].std(axis=1) / matches.iloc[:, 3:].mean(axis=1)) * 100).tolist(),\n",
    "            'SMILES_STD': chem_row.get('SMILES_ready', None),\n",
    "            'DTXSID': chem_row.get('DTXSID', None),\n",
    "            'CASRN': chem_row.get('CASRN', None),\n",
    "            'Compound Name': chem_row.get('PREFERRED_NAME', None),\n",
    "            'QC_NOTES': chem_row.get('QC_NOTES', None),\n",
    "            'InChiKey_origin': chem_row.get('INCHIKEY', None)\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for matched MSDIAL result rows\n",
    "matched_peak_area_df_pos = pd.DataFrame(matched_rows_pos)\n",
    "print(matched_peak_area_df_pos.shape)\n",
    "# Create a DataFrame for matched summary statistics\n",
    "matched_summary_df_pos = pd.DataFrame(matched_summary)\n",
    "matched_summary_df_pos.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/mathced_targetedmz_summary_forallsamples_pos_woRT.csv\")\n",
    "print(matched_summary_df_pos.shape)\n",
    "\n",
    "# Step 3: Handling technical duplicates and calculating mean values\n",
    "bh_columns = [col for col in matched_peak_area_df_pos.columns if col.startswith('BH')]\n",
    "mean_columns = {}\n",
    "for col in bh_columns:\n",
    "    base_col = col.split('_')[0]\n",
    "    if base_col not in mean_columns:\n",
    "        duplicate_cols = [c for c in bh_columns if c.startswith(base_col)]\n",
    "        mean_columns[base_col] = matched_peak_area_df_pos[duplicate_cols].mean(axis=1)\n",
    "\n",
    "# Retain only the columns with calculated mean values\n",
    "matched_peak_area_df_pos = pd.concat([matched_peak_area_df_pos.drop(columns=bh_columns), pd.DataFrame(mean_columns)], axis=1)\n",
    "\n",
    "# Step 4: Retain columns with median values > 5000, filter non-numeric values, and perform DataFrame transpose\n",
    "filtered_df = matched_peak_area_df_pos.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric values to NaN\n",
    "filtered_df = filtered_df.loc[:, (filtered_df.median(axis=0) > 5000)].dropna(how='all', axis=1)  # Retain columns with median value > 5000\n",
    "\n",
    "# Count the number of retained columns\n",
    "num_retained_columns = filtered_df.shape[1]\n",
    "print(f\"Number of columns retained after filtering: {num_retained_columns}\")\n",
    "\n",
    "# Calculate summary statistics for each sample column\n",
    "summary_stats = {\n",
    "    'Sum': filtered_df.sum(),\n",
    "    'Median': filtered_df.median(),\n",
    "    'Mean': filtered_df.mean(),\n",
    "    'Max': filtered_df.max(),\n",
    "    'Min': filtered_df.min(),\n",
    "    '95th Percentile': filtered_df.quantile(0.95),\n",
    "    '5th Percentile': filtered_df.quantile(0.05)\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the summary statistics\n",
    "summary_stats_df_pos = pd.DataFrame(summary_stats)\n",
    "summary_stats_df_pos.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/mathced_peaks_summary_forallsamples_pos_woRT.csv\")\n",
    "\n",
    "# Transpose the summary statistics DataFrame\n",
    "# transposed_summary_df = summary_stats_df_pos.transpose()\n",
    "# Display the final DataFrame\n",
    "# print(summary_stats_df_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yangj\\AppData\\Local\\Temp\\ipykernel_25660\\3624809644.py:10: DtypeWarning: Columns (18,19,20,23,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  targetmzdat_pos = pd.read_csv('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Plastic_map_chemicallist_match/Plastic_Chemical_List_organic_withRTprediction.csv')\n",
      "Matching compounds: 100%|██████████| 5144/5144 [01:26<00:00, 59.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340, 410)\n",
      "(535, 15)\n",
      "Number of columns retained after filtering: 122\n"
     ]
    }
   ],
   "source": [
    "##apply predicted RT\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load peak area data for positive ion mode after batch correction\n",
    "pos_peak_area_200 = pd.read_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/filtered_peak_area_after_bc_70.csv\")\n",
    "\n",
    "# Load target mz data\n",
    "# targetmzdat_pos = pd.read_csv('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Plastic_map_chemicallist_match/Plastic_Chemical_List_organic.csv')\n",
    "targetmzdat_pos = pd.read_csv('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Plastic_map_chemicallist_match/Plastic_Chemical_List_organic_withRTprediction.csv')\n",
    "targetmzdat_pos['mz'] = targetmzdat_pos['Monoisotopic_Mass_ready'] + 1.007825\n",
    "\n",
    "# Select relevant columns for inquiry\n",
    "peak_area_inquiry = pos_peak_area_200[['Average Rt(min)', 'Average Mz'] + [col for col in pos_peak_area_200.columns if col.startswith('BH')]]\n",
    "\n",
    "# Function to calculate ppm difference\n",
    "def ppm_difference(value1, value2):\n",
    "    return abs(value1 - value2) / value1 * 1e6\n",
    "\n",
    "def rt_diff(value1, valu2):\n",
    "    return (value1-valu2)\n",
    "\n",
    "ppm_threshold = 5\n",
    "matched_rows_pos = []\n",
    "matched_summary = []\n",
    "\n",
    "# MS1 search and matching\n",
    "for _, chem_row in tqdm(targetmzdat_pos.iterrows(), total=targetmzdat_pos.shape[0], desc=\"Matching compounds\"):\n",
    "    mw = chem_row['mz']\n",
    "    rtlow = chem_row['Lower Bound PI']\n",
    "    rthigh = chem_row['Upper Bound PI']\n",
    "    matches = peak_area_inquiry[peak_area_inquiry['Average Mz'].apply(lambda mz: ppm_difference(mz, mw) <= ppm_threshold)]\n",
    "    #print(f\"Matches after ppm filtering: {matches.shape}\")  # Debugging\n",
    "    if matches.empty:\n",
    "        continue\n",
    "    matches = matches[matches['Average Rt(min)'].apply(lambda rt: rt_diff(rt, rtlow) >=0)]\n",
    "    #print(f\"Matches after Rt lower filter:: {matches.shape}\")  # Debugging\n",
    "    if matches.empty:\n",
    "        continue\n",
    "    matches = matches[matches['Average Rt(min)'].apply(lambda rt: rt_diff(rt, rthigh) <=0)]\n",
    "    #print(f\"Matches after Rt high filter:: {matches.shape}\")  # Debugging\n",
    "    if matches.empty:\n",
    "        continue\n",
    "\n",
    "    if not matches.empty:\n",
    "        # Append the matches along with the corresponding compound information\n",
    "        for _, match_row in matches.iterrows():\n",
    "            matched_rows_pos.append({\n",
    "                **match_row,\n",
    "                'Matched Compound': chem_row['SMILES'],\n",
    "                'PREFERRED NAME': chem_row.get('PREFERRED_NAME', None),\n",
    "                'DTXSID_Hits': chem_row.get('DTXSID', None),\n",
    "                'SMILES_STD': chem_row.get('SMILES_ready', None),\n",
    "                'DTXSID': chem_row.get('DTXSID', None),\n",
    "                'Name': chem_row.get('CASRN', None),\n",
    "                'QC_NOTES': chem_row.get('QC_NOTES', None),\n",
    "                'InChiKey_origin': chem_row.get('INCHIKEY', None)\n",
    "            })\n",
    "        # Record summary statistics for each targeted mz, preserving multiple hits\n",
    "        matched_summary.append({\n",
    "            'Targeted MZ': mw,\n",
    "            'Matched Count': len(matches),\n",
    "            'Average Rt(min)': matches['Average Rt(min)'].tolist(),\n",
    "            'Average Mz': matches['Average Mz'].tolist(),\n",
    "            'Mean Value': matches.iloc[:, 3:].mean(axis=1).tolist(),\n",
    "            'Max Value': matches.iloc[:, 3:].max(axis=1).tolist(),\n",
    "            'Min Value': matches.iloc[:, 3:].min(axis=1).tolist(),\n",
    "            'Std Dev': matches.iloc[:, 3:].std(axis=1).tolist(),\n",
    "            'RSD (%)': ((matches.iloc[:, 3:].std(axis=1) / matches.iloc[:, 3:].mean(axis=1)) * 100).tolist(),\n",
    "            'SMILES_STD': chem_row.get('SMILES_ready', None),\n",
    "            'DTXSID': chem_row.get('DTXSID', None),\n",
    "            'CASRN': chem_row.get('CASRN', None),\n",
    "            'Compound Name': chem_row.get('PREFERRED_NAME', None),\n",
    "            'QC_NOTES': chem_row.get('QC_NOTES', None),\n",
    "            'InChiKey_origin': chem_row.get('INCHIKEY', None)\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for matched MSDIAL result rows\n",
    "matched_peak_area_df_pos = pd.DataFrame(matched_rows_pos)\n",
    "print(matched_peak_area_df_pos.shape)\n",
    "# Create a DataFrame for matched summary statistics\n",
    "matched_summary_df_pos = pd.DataFrame(matched_summary)\n",
    "matched_summary_df_pos.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/mathced_targetedmz_summary_forallsamples_pos.csv\")\n",
    "print(matched_summary_df_pos.shape)\n",
    "\n",
    "# Step 3: Handling technical duplicates and calculating mean values\n",
    "bh_columns = [col for col in matched_peak_area_df_pos.columns if col.startswith('BH')]\n",
    "mean_columns = {}\n",
    "for col in bh_columns:\n",
    "    base_col = col.split('_')[0]\n",
    "    if base_col not in mean_columns:\n",
    "        duplicate_cols = [c for c in bh_columns if c.startswith(base_col)]\n",
    "        mean_columns[base_col] = matched_peak_area_df_pos[duplicate_cols].mean(axis=1)\n",
    "\n",
    "# Retain only the columns with calculated mean values\n",
    "matched_peak_area_df_pos = pd.concat([matched_peak_area_df_pos.drop(columns=bh_columns), pd.DataFrame(mean_columns)], axis=1)\n",
    "\n",
    "# Step 4: Retain columns with median values > 5000, filter non-numeric values, and perform DataFrame transpose\n",
    "filtered_df = matched_peak_area_df_pos.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric values to NaN\n",
    "filtered_df = filtered_df.loc[:, (filtered_df.median(axis=0) > 5000)].dropna(how='all', axis=1)  # Retain columns with median value > 5000\n",
    "\n",
    "# Count the number of retained columns\n",
    "num_retained_columns = filtered_df.shape[1]\n",
    "print(f\"Number of columns retained after filtering: {num_retained_columns}\")\n",
    "\n",
    "# Calculate summary statistics for each sample column\n",
    "summary_stats = {\n",
    "    'Sum': filtered_df.sum(),\n",
    "    'Median': filtered_df.median(),\n",
    "    'Mean': filtered_df.mean(),\n",
    "    'Max': filtered_df.max(),\n",
    "    'Min': filtered_df.min(),\n",
    "    '95th Percentile': filtered_df.quantile(0.95),\n",
    "    '5th Percentile': filtered_df.quantile(0.05)\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the summary statistics\n",
    "summary_stats_df_pos = pd.DataFrame(summary_stats)\n",
    "summary_stats_df_pos.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/mathced_peaks_summary_forallsamples_pos.csv\")\n",
    "\n",
    "# Transpose the summary statistics DataFrame\n",
    "# transposed_summary_df = summary_stats_df_pos.transpose()\n",
    "# Display the final DataFrame\n",
    "# print(summary_stats_df_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank the feature by rt AND mz\n",
    "matched_peak_pos = matched_peak_area_df_pos.sort_values(by=['Average Rt(min)', 'Average Mz'])\n",
    "#filerted features with intensity lower than 5000 \n",
    "peak_area = matched_peak_pos[[col for col in matched_peak_pos.columns if col.startswith('BH')]]\n",
    "row_medians = peak_area.median(axis=1)\n",
    "peak_index = row_medians[row_medians >= 5000].index\n",
    "filter_results = matched_peak_pos.loc[peak_index]\n",
    "filter_results.shape\n",
    "filter_results.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/matched_peak_pos_over5000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_peak_pos.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/matched_peak_pos.csv\")\n",
    "#splitting the target list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching compounds: 100%|██████████| 5144/5144 [00:45<00:00, 113.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8098, 416)\n",
      "#of targetedmz with hits:(1507, 15)\n",
      "(1507, 15)\n",
      "(4302, 217)\n",
      "Number of columns retained after filtering: 134\n"
     ]
    }
   ],
   "source": [
    "#Negative alignment results\n",
    "from tqdm import tqdm\n",
    "# Load peak area data for positive ion mode after batch correction\n",
    "# pos_peak_area_200 = pd.read_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/filtered_peak_area_after_bc.csv\")\n",
    "# neg_peak_area_200 = pd.read_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/filtered_peak_area_after_bc_70.csv\")\n",
    "neg_peak_area_200 = pd.read_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/filtered_peak_area_for_bc_70.csv\")\n",
    "# Load target mz data\n",
    "targetmzdat_neg = pd.read_csv('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/plastic_map_chemlist/Plastic_Chemical_List_organic_withRTprediction_new_SMILES.csv')\n",
    "targetmzdat_neg['mz'] = targetmzdat_neg['MONOISOTOPIC_MASS'] - 1.007825\n",
    "\n",
    "# Select relevant columns for inquiry\n",
    "peak_area_inquiry = neg_peak_area_200[['Average Rt(min)', 'Average Mz'] + [col for col in neg_peak_area_200.columns if col.startswith('BH')]]\n",
    "\n",
    "# Function to calculate ppm difference\n",
    "def ppm_difference(value1, value2):\n",
    "    return abs(value1 - value2) / value1 * 1e6\n",
    "\n",
    "ppm_threshold = 5\n",
    "matched_rows_neg = []\n",
    "matched_summary = []\n",
    "\n",
    "# MS1 search and matching\n",
    "for _, chem_row in tqdm(targetmzdat_neg.iterrows(), total=targetmzdat_neg.shape[0], desc=\"Matching compounds\"):\n",
    "    mw = chem_row['mz']\n",
    "    matches = peak_area_inquiry[peak_area_inquiry['Average Mz'].apply(lambda mz: ppm_difference(mz, mw) <= ppm_threshold)]\n",
    "    #print(f\"Matches after ppm filtering: {matches.shape}\")  # Debugging\n",
    "    \n",
    "    if not matches.empty:\n",
    "        # Append the matches along with the corresponding compound information\n",
    "        for _, match_row in matches.iterrows():\n",
    "            matched_rows_neg.append({\n",
    "                **match_row,\n",
    "                'Matched Compound': chem_row['SMILES'],\n",
    "                'PREFERRED_NAME': chem_row.get('PREFERRED_NAME', None),\n",
    "                'DTXSID_Hits': chem_row.get('DTXSID', None),\n",
    "                'DTXSID': chem_row.get('DTXSID', None),\n",
    "                'InChiKey_origin': chem_row.get('INCHIKEY', None),\n",
    "                'MOLECULAR_FORMULA_original': chem_row.get('MOLECULAR_FORMULA', None),\n",
    "                'Compound_comment': chem_row.get('QC_NOTES', None),\n",
    "                'SMILES_STD': chem_row.get('SMILES_ready', None),\n",
    "                \"Monoisotopic_mass_ready\": chem_row.get('Monoisotopic_Mass_ready', None),\n",
    "                \"PredictedRT\": chem_row.get('Predicted RT', None),\n",
    "                \"PredictedRT_lower\": chem_row.get('Lower Bound PI', None),\n",
    "                \"PredictedRT_upper\": chem_row.get('Upper Bound PI', None),\n",
    "                'ENTACT_STD_RT' : chem_row.get('RT_ENTACT', None),\n",
    "                'DTSC_STD_RT': chem_row.get('RT_DTSC', None)\n",
    "            })\n",
    "        # Record summary statistics for each targeted mz, preserving multiple hits\n",
    "        matched_summary.append({\n",
    "            'Targeted MZ': mw,\n",
    "            'Matched Count': len(matches),\n",
    "            'Average Rt(min)': matches['Average Rt(min)'].tolist(),\n",
    "            'Average Mz': matches['Average Mz'].tolist(),\n",
    "            'Mean Value': matches.iloc[:, 3:].mean(axis=1).tolist(),\n",
    "            'Max Value': matches.iloc[:, 3:].max(axis=1).tolist(),\n",
    "            'Min Value': matches.iloc[:, 3:].min(axis=1).tolist(),\n",
    "            'Std Dev': matches.iloc[:, 3:].std(axis=1).tolist(),\n",
    "            'RSD (%)': ((matches.iloc[:, 3:].std(axis=1) / matches.iloc[:, 3:].mean(axis=1)) * 100).tolist(),\n",
    "            'SMILES_STD': chem_row.get('SMILES_ready', None),\n",
    "            'DTXSID': chem_row.get('DTXSID', None),\n",
    "            'CASRN': chem_row.get('CASRN', None),\n",
    "            'Compound Name': chem_row.get('PREFERRED_NAME', None),\n",
    "            'QC_NOTES': chem_row.get('QC_NOTES', None),\n",
    "            'InChiKey_origin': chem_row.get('INCHIKEY', None)\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for matched MSDIAL result rows\n",
    "matched_peak_area_df_neg = pd.DataFrame(matched_rows_neg)\n",
    "print(matched_peak_area_df_neg.shape)\n",
    "\n",
    "# Create a DataFrame for matched summary statistics\n",
    "matched_summary_df_neg = pd.DataFrame(matched_summary)\n",
    "print(f\"#of targetedmz with hits:{matched_summary_df_neg.shape}\")\n",
    "# matched_summary_df_neg.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/mathced_targetedmz_summary_forallsamples_Neg_woRT.csv\")\n",
    "print(matched_summary_df_neg.shape)\n",
    "\n",
    "# Step 3: Handling technical duplicates and calculating mean values\n",
    "bh_columns = [col for col in matched_peak_area_df_neg.columns if col.startswith('BH')]\n",
    "mean_columns = {}\n",
    "for col in bh_columns:\n",
    "    base_col = col.split('_')[0]\n",
    "    if base_col not in mean_columns:\n",
    "        duplicate_cols = [c for c in bh_columns if c.startswith(base_col)]\n",
    "        mean_columns[base_col] = matched_peak_area_df_neg[duplicate_cols].mean(axis=1)\n",
    "\n",
    "# Retain only the columns with calculated mean values\n",
    "matched_peak_area_df_neg = pd.concat([matched_peak_area_df_neg.drop(columns=bh_columns), pd.DataFrame(mean_columns)], axis=1)\n",
    "\n",
    "#output the matched peak area data\n",
    "#rank the feature by rt AND mz\n",
    "matched_peak_neg = matched_peak_area_df_neg.sort_values(by=['Average Rt(min)', 'Average Mz'])\n",
    "#filerted features with intensity lower than 5000 \n",
    "peak_areaneg = matched_peak_neg[[col for col in matched_peak_neg.columns if col.startswith('BH')]]\n",
    "row_mediansneg = peak_areaneg.median(axis=1)\n",
    "peak_indexneg = row_mediansneg[row_mediansneg >= 5000].index\n",
    "filter_resultsneg = matched_peak_neg.loc[peak_indexneg]\n",
    "print(filter_resultsneg.shape)\n",
    "# filter_resultsneg.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/matched_peak_neg_over5000_wopredRT.csv\")\n",
    "\n",
    "# Step 4: Retain columns with median values > 5000, filter non-numeric values, and perform DataFrame transpose\n",
    "filtered_df = matched_peak_area_df_neg.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric values to NaN\n",
    "filtered_df = filtered_df.loc[:, (filtered_df.median(axis=0) > 5000)].dropna(how='all', axis=1)  # Retain columns with median value > 5000\n",
    "\n",
    "# Count the number of retained columns\n",
    "num_retained_columns = filtered_df.shape[1]\n",
    "print(f\"Number of columns retained after filtering: {num_retained_columns}\")\n",
    "\n",
    "# Calculate summary statistics for each sample column for each individual samples\n",
    "#\n",
    "summary_stats = {\n",
    "    'Sum': filtered_df.sum(),\n",
    "    'Median': filtered_df.median(),\n",
    "    'Mean': filtered_df.mean(),\n",
    "    'Max': filtered_df.max(),\n",
    "    'Min': filtered_df.min(),\n",
    "    '95th Percentile': filtered_df.quantile(0.95),\n",
    "    '5th Percentile': filtered_df.quantile(0.05)\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the summary statistics\n",
    "summary_stats_df_neg = pd.DataFrame(summary_stats)\n",
    "# summary_stats_df_neg.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/mathced_peaks_summary_forallsamples_Neg_woRT.csv\")\n",
    "\n",
    "# Transpose the summary statistics DataFrame\n",
    "# transposed_summary_df_neg = summary_stats_df_neg.transpose()\n",
    "# Display the final DataFrame\n",
    "# print(summary_stats_df_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update on 20241211\n",
    "#check stock in ENTACKT mixture\n",
    "ENTACTmix = pd.read_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/plastic_map_chemlist/ENTACT_Mixture_chemlist.csv\")\n",
    "#add experimental RT to the matched peaks\n",
    "\n",
    "#check std availability and make annotation\n",
    "for iter, row in filter_resultsneg.iterrows():\n",
    "    if row['DTXSID_Hits'] in ENTACTmix['DTXSID'].tolist():\n",
    "        filter_resultsneg.loc[iter, 'ENTACT_std_availability'] = 'Yes'\n",
    "        # filter_resultsneg.loc[iter, 'ENTACT_std_RT'] = 'ENTACT_std_RT'\n",
    "    else:\n",
    "        filter_resultsneg.loc[iter, 'ENTACT_std_availability'] = 'No'\n",
    "\n",
    "filter_resultsneg.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/matched_peak_neg_over5000_wopredRT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1513, 218)\n",
      "(2789, 218)\n"
     ]
    }
   ],
   "source": [
    "filter_resultsneg = pd.read_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/matched_peak_neg_over5000_wopredRT.csv\")\n",
    "#make the suspects with predictedRT upper > average RT > predictedRT lower as the fisrt round of targeted msms analysis \n",
    "#make the suspects outside the predicted RT range as the second round of the targeted msms analysis\n",
    "First_round = filter_resultsneg[(filter_resultsneg['Average Rt(min)'] >= filter_resultsneg['PredictedRT_lower']) & (filter_resultsneg['Average Rt(min)'] <= filter_resultsneg['PredictedRT_upper'])]\n",
    "Second_round = filter_resultsneg[(filter_resultsneg['Average Rt(min)'] < filter_resultsneg['PredictedRT_lower']) | (filter_resultsneg['Average Rt(min)'] > filter_resultsneg['PredictedRT_upper'])]\n",
    "\n",
    "#import the first round results and cross check with the second round \n",
    "#add candidates with the same mz and rt from the seconde round to the first round \n",
    "print(First_round.shape)\n",
    "print(Second_round.shape)\n",
    "\n",
    "#update the first round with experimental rt from ENTACT RT library\n",
    "\n",
    "#update the first round with the experimental Record from previous results\n",
    "previous_first_round = pd.read_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/matched_peak_neg_over5000_firstround_msms.csv\")\n",
    "# # Frist_round_amend \n",
    "for iter, row in First_round.iterrows():\n",
    "    match_mz = First_round['Average Mz']\n",
    "    match_rt = First_round['Average Rt(min)']\n",
    "\n",
    "    if row['DTXSID'] in previous_first_round['DTXSID'].tolist():\n",
    "        First_round.loc[iter, 'Previous_MSMS'] = 'Yes'\n",
    "    else:\n",
    "        First_round.loc[iter, 'Previous_MSMS'] = 'No'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yangj\\AppData\\Local\\Temp\\ipykernel_9172\\456453554.py:9: DtypeWarning: Columns (18,19,20,23,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  targetmzdat_neg = pd.read_csv('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/plastic_map_chemlist/Plastic_Chemical_List_organic_withRTprediction.csv')\n",
      "Matching compounds: 100%|██████████| 5144/5144 [00:36<00:00, 140.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3006, 409)\n",
      "#of targetedmz with hits:(781, 15)\n",
      "(781, 15)\n",
      "(1546, 210)\n",
      "Number of columns retained after filtering: 106\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load peak area data for positive ion mode after batch correction\n",
    "# pos_peak_area_200 = pd.read_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/filtered_peak_area_after_bc.csv\")\n",
    "# neg_peak_area_200 = pd.read_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/filtered_peak_area_after_bc_70.csv\")\n",
    "neg_peak_area_200 = pd.read_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/filtered_peak_area_for_bc_70.csv\")\n",
    "# Load target mz data\n",
    "targetmzdat_neg = pd.read_csv('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/plastic_map_chemlist/Plastic_Chemical_List_organic_withRTprediction.csv')\n",
    "targetmzdat_neg['mz'] = targetmzdat_neg['MONOISOTOPIC_MASS'] - 1.007825\n",
    "\n",
    "# Select relevant columns for inquiry\n",
    "peak_area_inquiry = neg_peak_area_200[['Average Rt(min)', 'Average Mz'] + [col for col in neg_peak_area_200.columns if col.startswith('BH')]]\n",
    "\n",
    "# Function to calculate ppm difference\n",
    "def ppm_difference(value1, value2):\n",
    "    return abs(value1 - value2) / value1 * 1e6\n",
    "\n",
    "def rt_diff(value1, valu2):\n",
    "    return (value1-valu2)\n",
    "\n",
    "ppm_threshold = 5\n",
    "matched_rows_neg = []\n",
    "matched_summary = []\n",
    "\n",
    "# MS1 search and matching\n",
    "for _, chem_row in tqdm(targetmzdat_neg.iterrows(), total=targetmzdat_neg.shape[0], desc=\"Matching compounds\"):\n",
    "    mw = chem_row['mz']\n",
    "    rtlow = chem_row['Lower Bound PI']\n",
    "    rthigh = chem_row['Upper Bound PI']\n",
    "    matches = peak_area_inquiry[peak_area_inquiry['Average Mz'].apply(lambda mz: ppm_difference(mz, mw) <= ppm_threshold)]\n",
    "    #print(f\"Matches after ppm filtering: {matches.shape}\")  # Debugging\n",
    "    if matches.empty:\n",
    "        continue\n",
    "    matches = matches[matches['Average Rt(min)'].apply(lambda rt: rt_diff(rt, rtlow) >=0)]\n",
    "    #print(f\"Matches after Rt lower filter:: {matches.shape}\")  # Debugging\n",
    "    if matches.empty:\n",
    "        continue\n",
    "    matches = matches[matches['Average Rt(min)'].apply(lambda rt: rt_diff(rt, rthigh) <=0)]\n",
    "    #print(f\"Matches after Rt high filter:: {matches.shape}\")  # Debugging\n",
    "    if matches.empty:\n",
    "        continue\n",
    "    \n",
    "    if not matches.empty:\n",
    "        # Append the matches along with the corresponding compound information\n",
    "        for _, match_row in matches.iterrows():\n",
    "            matched_rows_neg.append({\n",
    "                **match_row,\n",
    "                'Matched Compound': chem_row['SMILES'],\n",
    "                'PREFERRED NAME': chem_row.get('PREFERRED NAME', None),\n",
    "                'DTXSID_Hits': chem_row.get('DTXSID', None),\n",
    "                'SMILES_STD': chem_row.get('SMILES_ready', None),\n",
    "                'DTXSID': chem_row.get('DTXSID', None),\n",
    "                'Name': chem_row.get('NAME', None),\n",
    "                'InChiKey_origin': chem_row.get('INCHIKEY', None)\n",
    "            })\n",
    "        # Record summary statistics for each targeted mz, preserving multiple hits\n",
    "        matched_summary.append({\n",
    "            'Targeted MZ': mw,\n",
    "            'Matched Count': len(matches),\n",
    "            'Average Rt(min)': matches['Average Rt(min)'].tolist(),\n",
    "            'Average Mz': matches['Average Mz'].tolist(),\n",
    "            'Mean Value': matches.iloc[:, 3:].mean(axis=1).tolist(),\n",
    "            'Max Value': matches.iloc[:, 3:].max(axis=1).tolist(),\n",
    "            'Min Value': matches.iloc[:, 3:].min(axis=1).tolist(),\n",
    "            'Std Dev': matches.iloc[:, 3:].std(axis=1).tolist(),\n",
    "            'RSD (%)': ((matches.iloc[:, 3:].std(axis=1) / matches.iloc[:, 3:].mean(axis=1)) * 100).tolist(),\n",
    "            'SMILES_STD': chem_row.get('SMILES_ready', None),\n",
    "            'DTXSID': chem_row.get('DTXSID', None),\n",
    "            'CASRN': chem_row.get('CASRN', None),\n",
    "            'Compound Name': chem_row.get('PREFERRED_NAME', None),\n",
    "            'QC_NOTES': chem_row.get('QC_NOTES', None),\n",
    "            'InChiKey_origin': chem_row.get('INCHIKEY', None)\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for matched MSDIAL result rows\n",
    "matched_peak_area_df_neg = pd.DataFrame(matched_rows_neg)\n",
    "print(matched_peak_area_df_neg.shape)\n",
    "\n",
    "# Create a DataFrame for matched summary statistics\n",
    "matched_summary_df_neg = pd.DataFrame(matched_summary)\n",
    "print(f\"#of targetedmz with hits:{matched_summary_df_neg.shape}\")\n",
    "# matched_summary_df_neg.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/mathced_targetedmz_summary_forallsamples_Neg.csv\")\n",
    "print(matched_summary_df_neg.shape)\n",
    "\n",
    "# Step 3: Handling technical duplicates and calculating mean values\n",
    "bh_columns = [col for col in matched_peak_area_df_neg.columns if col.startswith('BH')]\n",
    "mean_columns = {}\n",
    "for col in bh_columns:\n",
    "    base_col = col.split('_')[0]\n",
    "    if base_col not in mean_columns:\n",
    "        duplicate_cols = [c for c in bh_columns if c.startswith(base_col)]\n",
    "        mean_columns[base_col] = matched_peak_area_df_neg[duplicate_cols].mean(axis=1)\n",
    "\n",
    "# Retain only the columns with calculated mean values\n",
    "matched_peak_area_df_neg = pd.concat([matched_peak_area_df_neg.drop(columns=bh_columns), pd.DataFrame(mean_columns)], axis=1)\n",
    "\n",
    "#output the matched peak area data\n",
    "#rank the feature by rt AND mz\n",
    "matched_peak_neg = matched_peak_area_df_neg.sort_values(by=['Average Rt(min)', 'Average Mz'])\n",
    "#filerted features with intensity lower than 5000 \n",
    "peak_areaneg = matched_peak_neg[[col for col in matched_peak_neg.columns if col.startswith('BH')]]\n",
    "row_mediansneg = peak_areaneg.median(axis=1)\n",
    "peak_indexneg = row_mediansneg[row_mediansneg >= 5000].index\n",
    "filter_resultsneg = matched_peak_neg.loc[peak_indexneg]\n",
    "print(filter_resultsneg.shape)\n",
    "# filter_resultsneg.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/matched_peak_neg_over5000_wopredRT.csv\")\n",
    "\n",
    "# Step 4: Retain columns with median values > 5000, filter non-numeric values, and perform DataFrame transpose\n",
    "filtered_df = matched_peak_area_df_neg.apply(pd.to_numeric, errors='coerce')  # Convert non-numeric values to NaN\n",
    "filtered_df = filtered_df.loc[:, (filtered_df.median(axis=0) > 5000)].dropna(how='all', axis=1)  # Retain columns with median value > 5000\n",
    "\n",
    "# Count the number of retained columns\n",
    "num_retained_columns = filtered_df.shape[1]\n",
    "print(f\"Number of columns retained after filtering: {num_retained_columns}\")\n",
    "\n",
    "# Calculate summary statistics for each sample column for each individual samples\n",
    "summary_stats = {\n",
    "    'Sum': filtered_df.sum(),\n",
    "    'Median': filtered_df.median(),\n",
    "    'Mean': filtered_df.mean(),\n",
    "    'Max': filtered_df.max(),\n",
    "    'Min': filtered_df.min(),\n",
    "    '95th Percentile': filtered_df.quantile(0.95),\n",
    "    '5th Percentile': filtered_df.quantile(0.05)\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the summary statistics\n",
    "summary_stats_df_neg = pd.DataFrame(summary_stats)\n",
    "# summary_stats_df_neg.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/mathced_peaks_summary_forallsamples_Neg.csv\")\n",
    "\n",
    "# Transpose the summary statistics DataFrame\n",
    "# transposed_summary_df_neg = summary_stats_df_neg.transpose()\n",
    "# Display the final DataFrame\n",
    "# print(summary_stats_df_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 35)\n",
      "(781, 15)\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolToSmiles\n",
    "##cross refrence, cross check with blood exposome database\n",
    "bloodexpo_dat = pd.read_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/references/blood_exposome_database/blood_exposome_chemicals_july_2023.csv\")\n",
    "\n",
    "# Step 1: Filter rows with available MW (MONOISOTOPIC MASS) between 100 and 1000 and classified as organic\n",
    "bloodexpo_modified = []\n",
    "for index, row in bloodexpo_dat.iterrows():\n",
    "    smiles = row['CanonicalSMILES']  # Assuming the SMILES column is named 'SMILES'\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    canonical_smiles = Chem.MolToSmiles(mol, canonical=True) if mol else None\n",
    "    bloodexpo_modified.append({\n",
    "        **row,\n",
    "        \"SMILES_ready\": canonical_smiles})\n",
    "\n",
    "# Create a new DataFrame from the filtered rows\n",
    "bloodexpo_modified = pd.DataFrame(bloodexpo_modified)\n",
    "\n",
    "# Match SMILES from match_summary_df with bloodexpo_modified and retain hits\n",
    "# Match inchikey original between hits and the blood exposome database\n",
    "# matched_hits_SMILES = pd.merge(matched_summary_df_neg, bloodexpo_modified, how='inner', left_on='SMILES_STD', right_on='SMILES_ready')\n",
    "matched_hits_inchi = pd.merge(matched_summary_df_neg, bloodexpo_modified, how='inner', left_on='InChiKey_origin', right_on='InChIKey')\n",
    "#print(matched_hits_SMILES.shape)\n",
    "print(matched_hits_inchi.shape)\n",
    "# Match inchikey original between hits and the blood exposome database\n",
    "print(matched_summary_df_neg.shape)\n",
    "\n",
    "#title_dict = matched_hits_SMILES.groupby('SMILES_STD')['Title'].apply(list).to_dict()\n",
    "title_dict2 = matched_hits_inchi.groupby('InChiKey_origin')['Title'].apply(list).to_dict()\n",
    "papercount_dict = matched_hits_inchi.groupby('InChiKey_origin')['BloodPaperCount'].apply(list).to_dict()\n",
    "#matched_summary_df_neg['matched_with_bloodexpo_smi'] = matched_summary_df_neg['SMILES_STD'].map(title_dict).apply(lambda x: x if isinstance(x,list) else[])\n",
    "matched_summary_df_neg['matched_with_bloodexpo_inchi'] = matched_summary_df_neg['InChiKey_origin'].map(title_dict2).apply(lambda x: x if isinstance(x,list) else[])\n",
    "matched_summary_df_neg['BloodPaperCount'] = matched_summary_df_neg['InChiKey_origin'].map(papercount_dict).apply(lambda x: x if isinstance(x,list) else[])\n",
    "matched_summary_df_neg.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/mathced_targetedmz_summary_forallsamples_Neg.csv\")\n",
    "##annotate the targeted mz with the function information and the production volume information from the EST paper\n",
    "##when matching the plastic paper, some compounds are missing\n",
    "# plasticmap_est = pd.read_csv('D:/UCSF_postdoc_topic/REVEAL_topics/plastic_related_chemicals/plasticmap_from_ESTpaper.csv',encoding='ISO-8859-1')\n",
    "# # Match Inchikey from match_summary_df with bloodexpo_modified and retain hits\n",
    "# matched_summary_df_neg_plastic_note = pd.merge(matched_summary_df_neg, plasticmap_est, how='inner', left_on='InChiKey_origin', right_on='InChI_key')\n",
    "# matched_summary_df_neg_plastic_note.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/mathced_peaks_summary_withplasticannotation_Neg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 35)\n",
      "(535, 15)\n"
     ]
    }
   ],
   "source": [
    "# Match SMILES from match_summary_df with bloodexpo_modified and retain hits\n",
    "#matched_hits_SMILES = pd.merge(matched_summary_df_pos, bloodexpo_modified, how='inner', left_on='SMILES_STD', right_on='SMILES_ready')\n",
    "matched_hits_inchi = pd.merge(matched_summary_df_pos, bloodexpo_modified, how='inner', left_on='InChiKey_origin', right_on='InChIKey')\n",
    "\n",
    "# print(matched_hits_SMILES.shape)\n",
    "print(matched_hits_inchi.shape)\n",
    "# Match inchikey original between hits and the blood exposome database\n",
    "print(matched_summary_df_pos.shape)\n",
    "\n",
    "# title_dict = matched_hits_SMILES.groupby('SMILES_STD')['Title'].apply(list).to_dict()\n",
    "title_dict2 = matched_hits_inchi.groupby('InChiKey_origin')['Title'].apply(list).to_dict()\n",
    "papercount_dict = matched_hits_inchi.groupby('InChiKey_origin')['BloodPaperCount'].apply(list).to_dict()\n",
    "matched_summary_df_pos['matched_with_bloodexpo_inchi'] = matched_summary_df_pos['InChiKey_origin'].map(title_dict2).apply(lambda x: x if isinstance(x,list) else[])\n",
    "matched_summary_df_pos['BloodPaperCount'] = matched_summary_df_pos['InChiKey_origin'].map(papercount_dict).apply(lambda x: x if isinstance(x,list) else[])\n",
    "matched_summary_df_pos.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/mathced_targetedmz_summary_forallsamples_Pos.csv\")\n",
    "\n",
    "##annotate the targeted mz with the function information and the production volume information from the EST paper\n",
    "##when matching the plastic paper, some compounds are missing\n",
    "# plasticmap_est = pd.read_csv('D:/UCSF_postdoc_topic/REVEAL_topics/plastic_related_chemicals/plasticmap_from_ESTpaper.csv',encoding='ISO-8859-1')\n",
    "# # Match Inchikey from match_summary_df with bloodexpo_modified and retain hits\n",
    "# matched_summary_df_pos_plastic_note = pd.merge(matched_summary_df_pos, plasticmap_est, how='inner', left_on='InChiKey_origin', right_on='InChI_key')\n",
    "# matched_summary_df_pos_plastic_note.to_csv(\"D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/mathced_peaks_summary_withplasticannotation_pos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "##generate the files for CFM-ID spectra prediciton\n",
    "# CFMID_ready_df = matched_summary_df_neg[['SMILES_STD']].copy()\n",
    "# CFMID_ready_df['ID'] = ['Molecule' + str(i + 1) for i in range(len(CFMID_ready_df))]\n",
    "# CFMID_ready_df = CFMID_ready_df[['ID','SMILES_STD']]\n",
    "# ##CFMID_ready_df.to_csv('D:/UCSF_postdoc_topic/REVEAL_topics/First100_batch/neg_CFMID_MSready.txt', sep='\\t', index=False, header=False)\n",
    "# CFMID_ready_df.to_csv('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Neg_AlignmentResults/neg_CFMID_MSready.txt', sep='\\t', index=False, header=False)\n",
    "\n",
    "# #####\n",
    "# #generate the files for CFM-ID spectra prediciton\n",
    "# CFMID_ready_df = matched_summary_df_pos[['SMILES_STD']].copy()\n",
    "# CFMID_ready_df['ID'] = ['Molecule' + str(i + 1) for i in range(len(CFMID_ready_df))]\n",
    "# CFMID_ready_df = CFMID_ready_df[['ID','SMILES_STD']]\n",
    "# # CFMID_ready_df.to_csv('D:/UCSF_postdoc_topic/REVEAL_topics/First100_batch/neg_CFMID_MSready.txt', sep='\\t', index=False, header=False)\n",
    "# CFMID_ready_df.to_csv('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/Pos_AlignmentResults/Pos_CFMID_MSready.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable 'get_ipython' could not be pickled and was ignored.\n",
      "Variable 'exit' could not be pickled and was ignored.\n",
      "Variable 'quit' could not be pickled and was ignored.\n",
      "Variable 'open' could not be pickled and was ignored.\n",
      "Variable 'sys' could not be pickled and was ignored.\n",
      "Variable 'os' could not be pickled and was ignored.\n",
      "Variable 'np' could not be pickled and was ignored.\n",
      "Variable 'stats' could not be pickled and was ignored.\n",
      "Variable 'pd' could not be pickled and was ignored.\n",
      "Variable 'plt' could not be pickled and was ignored.\n",
      "Variable 'sb' could not be pickled and was ignored.\n",
      "Variable 'rd' could not be pickled and was ignored.\n",
      "Variable 'matplotlib' could not be pickled and was ignored.\n",
      "Variable 'warnings' could not be pickled and was ignored.\n",
      "Variable 'pickle' could not be pickled and was ignored.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os \n",
    "\n",
    "# Save all variables in the environment, excluding un-picklable ones\n",
    "with open('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/scripts/saved_environment.pkl', 'wb') as f:\n",
    "    save_dict = {}\n",
    "    for var_name, var_value in globals().items():\n",
    "        if not var_name.startswith('__') and var_name != 'f':\n",
    "            try:\n",
    "                pickle.dumps(var_value)\n",
    "                save_dict[var_name] = var_value\n",
    "            except (pickle.PicklingError, AttributeError, TypeError):\n",
    "                # Ignore un-picklable variables\n",
    "                print(f\"Variable '{var_name}' could not be pickled and was ignored.\")\n",
    "    pickle.dump(save_dict, f)\n",
    "\n",
    "print(\"Environment saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load all variables from the saved environment\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/scripts/saved_environment.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 15\u001b[0m     loaded_env \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Assign variables back to the global namespace\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mglobals\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(loaded_env)\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import rdkit \n",
    "from tqdm import tqdm\n",
    "\n",
    "def ppm_difference(value1, value2):\n",
    "    return abs(value1 - value2) / value1 * 1e6\n",
    "\n",
    "def rt_diff(value1, valu2):\n",
    "    return (value1-valu2)\n",
    "\n",
    "# Load all variables from the saved environment\n",
    "with open('D:/UCSF_postdoc_topic/REVEAL_topics/REVEAL_200samples_analysis/scripts/saved_environment.pkl', 'rb') as f:\n",
    "    loaded_env = pickle.load(f)\n",
    "\n",
    "# Assign variables back to the global namespace\n",
    "globals().update(loaded_env)\n",
    "\n",
    "print(\"Environment loaded successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "echoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
